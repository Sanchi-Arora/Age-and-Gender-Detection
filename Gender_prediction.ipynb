{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transfer_learning-VGG16.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xry9F4ar1jzP","outputId":"99ed1037-b22f-48fa-8726-ebc588372ab5"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"n9BoU7K90ugY","colab":{"base_uri":"https://localhost:8080/"},"outputId":"15c285ad-646b-4d84-d708-9e3d859ba530"},"source":["from keras.preprocessing.image import ImageDataGenerator\n","\n","train_dir = \"/content/drive/MyDrive/gender/Training/\"\n","test_dir = \"/content/drive/MyDrive/gender/Validation/\"\n","\n","\n","# Using imagedatagenerator to access the files and image augmentation.\n","\n","# You can change the setting of imageDataGenerator for other augmentation methods.\n","\n","train_datagen = ImageDataGenerator(rescale=1/255)\n","test_datagen = ImageDataGenerator(rescale=1/255)\n","\n","\n","training_generator = train_datagen.flow_from_directory(train_dir,\n","                                                      target_size=(224, 224),\n","                                                      class_mode=\"binary\")\n","\n","\n","test_generator = test_datagen.flow_from_directory(test_dir,\n","                                                       target_size=(224, 224),\n","                                                       class_mode=\"binary\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 22667 images belonging to 2 classes.\n","Found 5472 images belonging to 2 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"s589jIyS06gl"},"source":["from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Flatten, Dense, Dropout\n","from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n","from tensorflow.keras.applications.vgg16 import VGG16\n","from tensorflow.keras.applications.vgg16 import preprocess_input\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.optimizers import Adam\n","import keras\n","import os\n","import shutil\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BAB4YKGY09u2"},"source":["target_size = (224, 224)\n","batch_size = 128\n","mode = 'binary'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pj6KpGu71A5W","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0a8e1768-b23d-41a8-cb7b-398a8354fcb2"},"source":["vgg = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58892288/58889256 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zGM2YcPu1Bfw"},"source":["vgg.trainable = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9chCd_lG1BqO"},"source":["model_with_tuning = 'transfer_model.h5'\n","checkpoint_callback = ModelCheckpoint(model_with_tuning,\n","                                     monitor='val_accuracy',\n","                                     save_best_only=True,\n","                                     verbose=1)\n","earlystop = EarlyStopping(monitor='val_accuracy',patience=5,verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S15yV5Y51JR1"},"source":["model = Sequential()\n","model.add(vgg)\n","model.add(Flatten())\n","model.add(Dense(256, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZjFiL-sA1JCU"},"source":["model.compile(loss='binary_crossentropy',\n","             metrics=['accuracy'],\n","             optimizer='adam')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZBhqbdFm1Nxu","colab":{"base_uri":"https://localhost:8080/"},"outputId":"110e983f-5ea3-4fc3-8d8a-a897088a6804"},"source":["history_without_tuning = model.fit(training_generator,\n","                   steps_per_epoch=22667  // batch_size,\n","                   epochs=10,\n","                   validation_data=test_generator,\n","                   validation_steps=5472 // batch_size,\n","                   verbose=1,\n","                   callbacks = [checkpoint_callback,earlystop])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","177/177 [==============================] - 2565s 14s/step - loss: 0.7228 - accuracy: 0.7874 - val_loss: 0.2150 - val_accuracy: 0.9144\n","\n","Epoch 00001: val_accuracy improved from -inf to 0.91443, saving model to transfer_model.h5\n","Epoch 2/10\n","177/177 [==============================] - 2043s 12s/step - loss: 0.1820 - accuracy: 0.9315 - val_loss: 0.1603 - val_accuracy: 0.9345\n","\n","Epoch 00002: val_accuracy improved from 0.91443 to 0.93452, saving model to transfer_model.h5\n","Epoch 3/10\n","177/177 [==============================] - 1403s 8s/step - loss: 0.1509 - accuracy: 0.9462 - val_loss: 0.1724 - val_accuracy: 0.9390\n","\n","Epoch 00003: val_accuracy improved from 0.93452 to 0.93899, saving model to transfer_model.h5\n","Epoch 4/10\n","177/177 [==============================] - 1177s 7s/step - loss: 0.1521 - accuracy: 0.9440 - val_loss: 0.2075 - val_accuracy: 0.9182\n","\n","Epoch 00004: val_accuracy did not improve from 0.93899\n","Epoch 5/10\n","177/177 [==============================] - 790s 4s/step - loss: 0.1621 - accuracy: 0.9382 - val_loss: 0.1247 - val_accuracy: 0.9501\n","\n","Epoch 00005: val_accuracy improved from 0.93899 to 0.95015, saving model to transfer_model.h5\n","Epoch 6/10\n","177/177 [==============================] - 649s 4s/step - loss: 0.1305 - accuracy: 0.9525 - val_loss: 0.1454 - val_accuracy: 0.9427\n","\n","Epoch 00006: val_accuracy did not improve from 0.95015\n","Epoch 7/10\n","177/177 [==============================] - 437s 2s/step - loss: 0.1199 - accuracy: 0.9571 - val_loss: 0.1357 - val_accuracy: 0.9546\n","\n","Epoch 00007: val_accuracy improved from 0.95015 to 0.95461, saving model to transfer_model.h5\n","Epoch 8/10\n","177/177 [==============================] - 379s 2s/step - loss: 0.1205 - accuracy: 0.9590 - val_loss: 0.1576 - val_accuracy: 0.9353\n","\n","Epoch 00008: val_accuracy did not improve from 0.95461\n","Epoch 9/10\n","177/177 [==============================] - 265s 2s/step - loss: 0.1129 - accuracy: 0.9572 - val_loss: 0.2128 - val_accuracy: 0.9271\n","\n","Epoch 00009: val_accuracy did not improve from 0.95461\n","Epoch 10/10\n","177/177 [==============================] - 214s 1s/step - loss: 0.1446 - accuracy: 0.9477 - val_loss: 0.1332 - val_accuracy: 0.9479\n","\n","Epoch 00010: val_accuracy did not improve from 0.95461\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kBI9b6011OLd"},"source":["model.save('/content/drive/MyDrive/transfer_model.h5')\n","#model = keras.models.load_model('/content/drive/MyDrive/transfer_model_model.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cFXhluT116dk","outputId":"1da95d26-995d-4a33-b600-77845bf2b885"},"source":["from keras.preprocessing.image import ImageDataGenerator\n","\n","test = \"/content/drive/MyDrive/pic/\"\n","\n","\n","test_data = ImageDataGenerator(rescale=1/255)\n","\n","\n","test_generator = test_data.flow_from_directory(test,\n","                                              target_size=(224, 224),\n","                                              class_mode=\"binary\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 20 images belonging to 2 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WcvQqfDj4RGq","colab":{"base_uri":"https://localhost:8080/"},"outputId":"97334fb6-fd29-4ccf-d334-c5748a5d02e3"},"source":["test_loss, test_acc = model.evaluate(test_generator)\n","print('test acc:', test_acc)\n","print('test_loss:',test_loss)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1/1 [==============================] - 1s 1s/step - loss: 0.0034 - accuracy: 1.0000\n","test acc: 1.0\n","test_loss: 0.003373892279341817\n"],"name":"stdout"}]}]}